# LACN Operations Survey — Getting Started Guide

> **Last updated:** February 2026 (for the 2025–2026 survey cycle)
>
> This guide walks the next person through setting up, running, and deploying the LACN Operations Survey reports. Follow it **step by step** with new survey data (e.g., 2027).

---

## Table of Contents

1. [Project Overview](#1-project-overview)
2. [Prerequisites](#2-prerequisites)
3. [Repository Structure](#3-repository-structure)
4. [Step-by-Step: Processing New Data](#4-step-by-step-processing-new-data)
5. [Step-by-Step: Generating Reports](#5-step-by-step-generating-reports)
6. [Step-by-Step: Deploying to GitHub Pages](#6-step-by-step-deploying-to-github-pages)
7. [Common Issues & Troubleshooting](#7-common-issues--troubleshooting)
8. [Year-to-Year Checklist](#8-year-to-year-checklist)

---

## 1. Project Overview

This project processes the annual **LACN (Liberal Arts Career Network) Operations Survey** data and generates:

- **General Report** — aggregate statistics across all participating institutions
- **Custom Reports** — one HTML report per institution, showing their data against the aggregate

The pipeline reads a raw Qualtrics CSV export, cleans and analyzes the data in R, and renders parameterized R Markdown reports as HTML files hosted on GitHub Pages.

---

## 2. Prerequisites

### Software Required

| Software | Version Used | How to Install |
|----------|-------------|---------------|
| **R** | 4.5.2+ | [r-project.org](https://cran.r-project.org/) |
| **RStudio** (optional) | Latest | [posit.co](https://posit.co/download/rstudio-desktop/) |
| **Git** | Any recent | `brew install git` (macOS) |
| **Pandoc** | 3.0+ | Bundled with RStudio, or `brew install pandoc` |

### R Packages Required

Open R or RStudio and run:

```r
install.packages(c(
  "tidyverse",    # Core data manipulation + ggplot2
  "rmarkdown",    # Report rendering
  "knitr",        # Knitting engine
  "kableExtra",   # Styled HTML tables
  "ggrepel",      # Non-overlapping text labels on charts
  "showtext",     # Google Fonts in ggplot2
  "sysfonts",     # Font management for showtext
  "scales"        # Axis label formatting (dollar signs, percentages)
))
```

---

## 3. Repository Structure

```
lacn2025/
├── code/                          # All R scripts (run in order via source.R)
│   ├── source.R                   # ★ MASTER SCRIPT — runs everything
│   ├── 1_read_data.R              # Reads raw CSV, creates response_key
│   ├── 2_clean.R                  # Splits data into per-question dataframes
│   ├── 3_functions.R              # Visualization helper functions
│   ├── 99_processing_functions.R  # Analysis functions (frequency, mean, etc.)
│   ├── 99_processing.R            # Runs analysis across all questions
│   ├── 4_viz_intro.R              # Enrollment visualization data
│   ├── 5_viz_reporting.R          # Reporting structure viz data
│   ├── 6_viz_services.R           # Services & programs viz data
│   ├── 7_viz_employer.R           # Employer relations viz data
│   ├── 8_viz_engagement.R         # Student engagement viz data
│   ├── 9_viz_budget.R             # Budget viz data
│   ├── 99_custom_exe.R            # ★ BATCH RENDER all custom reports
│   └── 99_email_sharing.R         # (Optional) Email distribution script
│
├── data/                          # Raw survey data + lookup tables
│   ├── OpsSurveyRawData2.27.26.csv    # ← Current raw data file
│   ├── response_key_26.csv            # ← Cleaned question/response labels
│   ├── question_type_26.csv           # ← Question type classifications
│   └── (older files from prior years)
│
├── docs/                          # Generated HTML reports (output)
│   ├── GeneralReport.html         # Aggregate report
│   ├── St_Olaf_College.html       # Example custom report
│   ├── custom_template.Rmd        # ★ Template for custom reports
│   ├── GeneralReport.Rmd          # General report template
│   └── style.css                  # CSS styling
│
├── lacn.RData                     # Saved R environment (generated by source.R)
├── _site.yml                      # GitHub Pages navigation config
├── index.Rmd                      # Landing page
└── README.md                      # Original project documentation (2022)
```

### Key Files You'll Edit Each Year

| File | What to Change |
|------|---------------|
| `code/1_read_data.R` | CSV filename, column removal range |
| `code/2_clean.R` | Column counts for Q1, Q2, Q3 if questions changed |
| `data/response_key_XX.csv` | New response labels (generated then manually cleaned) |
| `data/question_type_XX.csv` | Question type classifications |

---

## 4. Step-by-Step: Processing New Data

### Step 1: Add the Raw CSV

1. Download the raw survey data from Qualtrics as a CSV
2. Place it in the `data/` folder
3. Name it descriptively, e.g., `OpsSurveyRawData3.15.27.csv`

### Step 2: Update `code/1_read_data.R`

Open this file and make these changes:

**a) Update the CSV filename (line ~6):**
```r
# CHANGE THIS to your new file name:
lacn_location <- file.path("data", "OpsSurveyRawData3.15.27.csv")
```

**b) Update the duplicate column removal range (line ~16):**

The raw Qualtrics export often has duplicate IPEDS columns at the end. You need to find and remove them.

```r
# Check which columns are duplicated by running in R console:
temp <- readr::read_csv("data/YOUR_NEW_FILE.csv")
# Look at the last ~20 column names:
tail(names(temp), 20)
# Find duplicated columns and update the range:
lacn_master <- readr::read_csv(lacn_location, col_select = c(-(START:END)))
```

> **Tip:** Use `identical(temp$col1, temp$col2)` to verify columns are truly duplicates before removing.

**c) Update institution name corrections (line ~28):**

Check if any institution names need correction:
```r
# After reading data, check unique institution names:
sort(unique(lacn_master$`Institution Name`))
```

**d) Update response_key filenames (lines ~53-61):**
```r
# Save messy version:
readr::write_csv(response_key_messy, file.path("data/response_key_messy_27.csv"))

# After manual cleaning, load the clean one:
response_key <- readr::read_csv(file.path("data/response_key_27.csv"))

# Also update question_type:
question_type <- readr::read_csv(file.path("data/question_type_27.csv"))
```

### Step 3: Create `response_key_XX.csv`

This is the most **manual** step. The response key maps each question sub-column to meaningful labels.

1. **Run `1_read_data.R` up to line ~52** to generate `response_key_messy_XX.csv`
2. Open `response_key_messy_XX.csv` in Excel/Google Sheets
3. You need to fill in these columns for each question:
   - `main` — the parent question (e.g., Q7)
   - `sub1` — sub-question number
   - `sub2` — secondary sub-question (for matrix questions)
   - `dim1` — primary dimension label (used as chart axis)
   - `dim2` — secondary dimension label (used as facet/legend)
4. **Start from last year's file** (`response_key_26.csv`) as a template — most questions stay the same year to year. Copy over the `dim1` and `dim2` columns for questions that haven't changed.
5. Save as `data/response_key_27.csv`

### Step 4: Create `question_type_XX.csv`

This file tells the pipeline how to analyze each question. It has 3 columns:

| unique | q_type | sub_num |
|--------|--------|---------|
| Q1 | text | 3 |
| Q2 | multi | 6 |
| Q3 | single | 1 |
| Q5 | matrix | 30 |
| Q6 | continuous | 6 |
| ... | ... | ... |

**Question types:**
- `text` — free text (not analyzed)
- `single` — single-select multiple choice
- `multi` — multi-select (check all that apply)
- `matrix` — grid/matrix question
- `continuous` — numeric value

> **Tip:** Copy last year's `question_type_26.csv` and update only questions that changed.

### Step 5: Update `code/2_clean.R`

Check if the number of sub-columns changed for Q1, Q2, or Q3:

```r
# Q1: usually 5 columns (Institution + Enrollment + Q1_1 + Q1_3 + Q1_2)
question_list$Q1 <- question_list$Q1[, 1:5]

# Q2: count how many Q2_X columns exist in the new data
#     In 2026 it was 6 multi-select options → [1:9] (Institution + Enrollment + 7 Q2 cols)
#     Previously it was single-select → [1:3]
question_list$Q2 <- question_list$Q2[, 1:9]  # ADJUST if Q2 sub-columns changed

# Q3: usually 3 columns (Institution + Enrollment + Q3)
question_list$Q3 <- question_list$Q3[, 1:3]
```

### Step 6: Run the Pipeline

```bash
cd lacn2025
Rscript code/source.R
```

This runs all scripts 1 through 9 and saves the processed data to `lacn.RData`.

> **If it errors**, see the [Troubleshooting](#7-common-issues--troubleshooting) section.

---

## 5. Step-by-Step: Generating Reports

### General Report

```r
rmarkdown::render("docs/GeneralReport.Rmd")
```

### Single Custom Report (for testing)

```r
rmarkdown::render(
  "docs/custom_template.Rmd",
  output_file = "St_Olaf_College.html",
  output_dir = "docs",
  params = list(college = "St Olaf College")
)
```

> **Important:** The `college` parameter must exactly match the institution name in the data. Run `sort(unique(question_list$Q1$'Institution Name'))` to see all valid names.

### Batch Render All Custom Reports

```r
source("code/99_custom_exe.R")
```

This loops through all institutions and generates one HTML file per school in `docs/custom/`.

> **Note:** The batch script starts from index 37 (`colleges <- colleges[37:length(colleges)]`). This skips the header row. Verify this index is correct for your data — it should start at the first actual institution row.

---

## 6. Step-by-Step: Deploying to GitHub Pages

1. **Commit and push** all changes:
   ```bash
   git add -A
   git commit -m "2027 survey reports"
   git push origin main
   ```

2. **Enable GitHub Pages** (if not already):
   - Go to your repo on GitHub → Settings → Pages
   - Source: Deploy from branch → `main` → `/docs` folder
   - Click Save

3. Reports will be available at:
   ```
   https://your-username.github.io/lacn2025/
   ```

---

## 7. Common Issues & Troubleshooting

### ❌ "Font family not found" or garbled text

The project uses **Source Sans 3** (formerly "Source Sans Pro") from Google Fonts. The font is loaded via the `sysfonts` and `showtext` packages.

**Fix:** Make sure these lines are in the setup chunk of your Rmd files:
```r
sysfonts::font_add_google("Source Sans 3", "Source Sans 3")
showtext::showtext_auto()
showtext::showtext_opts(dpi = 96)
```

If the Google Fonts API renames the font again, check `sysfonts::font_families_google()` for the current name.

### ❌ Oversized text in charts / overlapping labels

This happens when the `showtext` DPI doesn't match `knitr`'s output DPI. Ensure the Rmd setup chunk has:
```r
knitr::opts_chunk$set(fig.showtext = TRUE, fig.retina = 1)
showtext::showtext_opts(dpi = 96)
```

Chart text `size` parameters should generally be in the 20–25 range for HTML output (not 45, which was for print/PDF).

### ❌ "Column doesn't exist" or index out of bounds

The column removal range in `1_read_data.R` is **data-specific**. If the survey adds/removes questions, the duplicate IPEDS columns shift. Always verify the range with:
```r
temp <- readr::read_csv("data/YOUR_FILE.csv")
tail(names(temp), 20)  # Look for duplicated column names at the end
```

### ❌ Google Sheets authentication errors

The 2026 version removed the Google Sheets dependency. Response keys and question types are loaded from local CSV files. You should NOT need `googlesheets4` at all.

### ❌ Q2 changed from single-select to multi-select (or vice versa)

In 2026, Q2 changed from a single-select to multi-select question. If this happens again:
1. Update `question_type_XX.csv`: change the `q_type` for Q2
2. Update `2_clean.R`: adjust the column count for Q2
3. The `custom_template.Rmd` Q2 chart code handles multi-select via pivot_longer

---

## 8. Year-to-Year Checklist

Use this checklist when processing a new year's data:

- [ ] **Get the raw CSV** from Qualtrics, put in `data/`
- [ ] **Update `1_read_data.R`:**
  - [ ] Change CSV filename
  - [ ] Check and update duplicate column removal range
  - [ ] Check institution name corrections
  - [ ] Update response_key and question_type filenames to new year
- [ ] **Generate response key:**
  - [ ] Run `1_read_data.R` up to `response_key_messy` creation
  - [ ] Copy last year's `response_key_XX.csv` as a starting point
  - [ ] Update `dim1`/`dim2` labels for any new or changed questions
  - [ ] Save as `data/response_key_XX.csv`
- [ ] **Create `question_type_XX.csv`:**
  - [ ] Copy last year's file
  - [ ] Update any questions that changed type (single ↔ multi, etc.)
- [ ] **Update `2_clean.R`:**
  - [ ] Verify Q1, Q2, Q3 column counts match new data
- [ ] **Run pipeline:** `Rscript code/source.R`
- [ ] **Test one report:** Render a single custom report and verify charts
- [ ] **Batch render:** Run `99_custom_exe.R` for all institutions
- [ ] **Knit general report:** Render `GeneralReport.Rmd`
- [ ] **Commit, push, verify GitHub Pages**

---

## Questions?

If you get stuck, the most helpful resource is the existing code itself. Each script has inline comments, and the numbered naming convention (`1_`, `2_`, `3_`...) shows the execution order. Start with `source.R` to see the full pipeline flow.
